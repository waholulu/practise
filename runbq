import logging
from google.cloud import bigquery
import os

# Initialize client for BigQuery
bq_client = bigquery.Client()

# Parameters
local_folder_path = '/path/to/your/sql/files/'
project_id = 'your-project-id'
dataset_id = 'your-dataset-id'
log_file = 'query_execution.log'

# Predefined list of file names, ordered as required
predefined_file_list = [
    '001_first_query.bq',
    '002_second_query.bq',
    '003_third_query.bq'
    # Add more file names as needed
]

# Set up logging
logging.basicConfig(filename=log_file, level=logging.INFO,
                    format='%(asctime)s - %(levelname)s - %(message)s')

def execute_query_from_file(file_path):
    """Execute a BigQuery query from a local file and log the execution."""
    with open(file_path, 'r') as file:
        query_string = file.read()
    logging.info(f"Executing query from {os.path.basename(file_path)}...")
    try:
        query_job = bq_client.query(query_string, project=project_id)
        query_job.result()  # Wait for the query to finish
        logging.info(f"Query from {os.path.basename(file_path)} executed successfully.")
    except Exception as e:
        logging.error(f"Failed to execute query from {os.path.basename(file_path)}: {str(e)}")

def main():
    # Execute each query file in the predefined order
    for file_name in predefined_file_list:
        file_path = os.path.join(local_folder_path, file_name)
        execute_query_from_file(file_path)

if __name__ == '__main__':
    main()
