Random Under Sampling: Reduces the size of the over-represented class by randomly discarding its samples, balancing class distribution. This method is straightforward but risks losing important data.

Instance Hardness Threshold (IHT): Eliminates hard-to-classify samples from the majority class, keeping only those that are easy to classify. It aims to retain significant data while balancing the dataset.

NearMiss: A set of methods that under-sample the majority class based on the distance to minority class samples. It has different versions, each selecting majority class samples based on varying criteria of closeness to minority class samples.


Random Over Sampling: Increases the size of the under-represented class by duplicating its samples. Simple but can lead to overfitting due to repetition.

SMOTE (Synthetic Minority Over-sampling Technique): Creates synthetic samples for the minority class by interpolating between existing samples. Adds diversity, reducing overfitting risks compared to random over-sampling.

SMOTENC (SMOTE for Nominal and Continuous): An adaptation of SMOTE for datasets with both nominal (categorical) and continuous features. Generates synthetic samples considering both types of features, suitable for mixed-type data.


Backward Sequential Feature Selection (BSFS): Starts with all features, then iteratively removes the least significant ones to improve model performance. Continues until a set number of features is left or no further improvement is seen.

Recursive Feature Elimination (RFE): Similar to BSFS but relies on a model's feature importance to eliminate features. Repeatedly removes the least important features, building a model each time, until reaching the desired number of features.


Optuna is a Python library for efficient hyperparameter optimization in machine learning. It automates the selection of optimal hyperparameters using advanced algorithms, supports early stopping of unpromising trials (pruning), and offers tools for easy analysis and visualization of results. Optuna is known for its user-friendly approach and computational efficiency.



Bias Towards Majority Class: When there's a significant imbalance in the data, the model may become biased towards the majority class. It might perform well on the majority class while failing to accurately predict the minority class. This happens because the model has not been exposed to enough examples of the minority class to learn from.

Poor Generalization: A model trained on imbalanced data may not generalize well to new, unseen data, especially if the new data has a different distribution. The model's ability to predict the minority class is often poor, which can be critical if the minority class is of greater interest (e.g., in fraud detection or disease diagnosis).

Inaccurate Performance Metrics: Standard performance metrics like accuracy can be misleading in the context of imbalanced datasets. A model might achieve high accuracy by simply predicting the majority class all the time, but this doesn't mean it's performing well on the task it's supposed to solve, especially if the minority class is important.

Overfitting Risk: There's a risk that the model might overfit to the majority class. Since there are more examples of this class, the model might end up learning noise in the majority class data instead of generalizing from the underlying patterns.



Iterations (also known as Trees or N_estimators): This is the number of trees to be built in the model. More trees can improve the model's accuracy but also increase the computation time and risk of overfitting.

Depth: This parameter defines the depth of each tree. A deeper tree can model more complex relationships but also increases the risk of overfitting and requires more computational resources.

Learning Rate: Also known as the shrinkage or eta in other algorithms, this parameter controls the contribution of each tree to the final prediction. A smaller learning rate requires more trees to model all the relationships but can lead to a more accurate and generalized model.

Random Strength: This parameter is used in the splitting algorithm, particularly for dealing with categorical features. It specifies the amount of randomness to be used for choosing features to split on, helping in preventing overfitting.

Bagging Temperature: Used in the Bayesian bootstrap, this parameter controls the degree of randomness for bagging. A higher value provides more diverse subsamples, which can help in reducing overfitting.

Border Count: This is used in the quantization of continuous features. It represents the number of splits considered for each continuous feature. Higher values can lead to more precise splits but increase computation time and overfitting risk.

Leaf Estimation Iterations: This is the number of iterations used in the optimization of the values in the leaves. More iterations can lead to a more accurate model but can also cause overfitting and increase computation time.

Scale Pos Weight: This parameter is crucial for imbalanced datasets. Itâ€™s used to scale the weight of the positive class (usually the minority class) during training to combat the imbalance.

OD Wait (Overfitting Detector Wait): This parameter is part of the early stopping mechanism in CatBoost. It defines the number of iterations to continue the training after the model's performance has stopped improving. A smaller value can prevent overfitting but might stop training too early, while a larger value can lead to overfitting.


Grid Search:

Approach: Exhaustively searches through a predefined grid of hyperparameters.
Pros: Simple and guarantees finding the best combination within the grid.
Cons: Computationally expensive and limited to specified parameter ranges.
Optuna:

Approach: Uses Bayesian optimization to efficiently search the hyperparameter space.
Pros: More efficient than Grid Search, flexible in handling various parameter types.
Cons: More complex and doesn't exhaustively evaluate all combinations.
