import optuna
from catboost import CatBoostRegressor, Pool
import numpy as np
import pandas as pd

# Assume X_train, y_train are your training features and target variable respectively
# Assume X_valid, y_valid are your validation features and target variable respectively
# Replace these with your actual data
X_train = np.random.rand(1000, 400)  # Dummy data, replace with your actual training data
y_train = np.random.rand(1000)       # Dummy data, replace with your actual training data
X_valid = np.random.rand(200, 400)   # Dummy data, replace with your actual validation data
y_valid = np.random.rand(200)        # Dummy data, replace with your actual validation data

# Create Pool objects for the training and validation sets
train_pool = Pool(X_train, y_train)
valid_pool = Pool(X_valid, y_valid)

def objective(trial):
    # Define the hyperparameter configuration space
    param = {
        'iterations': trial.suggest_int('iterations', 100, 1000),
        'depth': trial.suggest_int('depth', 6, 10),
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),
        'random_strength': trial.suggest_int('random_strength', 1, 20),
        'bagging_temperature': trial.suggest_float('bagging_temperature', 0, 1),
        'l2_leaf_reg': trial.suggest_int('l2_leaf_reg', 1, 10),
        'border_count': trial.suggest_int('border_count', 32, 255),
        'ctr_border_count': trial.suggest_int('ctr_border_count', 50, 200),
        'leaf_estimation_iterations': trial.suggest_int('leaf_estimation_iterations', 1, 10),
        'bootstrap_type': trial.suggest_categorical('bootstrap_type', ['Bayesian', 'Bernoulli', 'MVS']),
        'subsample': trial.suggest_float('subsample', 0.5, 1) if trial.suggest_categorical('bootstrap_type', ['Bernoulli', 'MVS']) else None,
        'loss_function': 'RMSE',
        'eval_metric': 'RMSE',
        'od_type': 'Iter',
        'od_wait': 50
    }
    
    # Instantiate the CatBoost model
    catboost_model = CatBoostRegressor(**param)
    
    # Fit model with the training set and evaluate it on the validation set
    catboost_model.fit(train_pool, eval_set=valid_pool, verbose=0, plot=False)
    
    # Get the evaluation results
    evals_result = catboost_model.get_evals_result()
    
    # Return the last RMSE value from the evaluation results on the validation set
    # Optuna tries to minimize the objective, so if you have a different metric, ensure it's being minimized
    rmse = evals_result['validation']['RMSE'][-1]
    
    return rmse

# Create a study object and specify the direction is 'minimize'.
study = optuna.create_study(direction='minimize')

# Start the hyperparameter optimization
study.optimize(objective, n_trials=50)  # You can increase n_trials for a thorough search

# Fetch the best hyperparameters
best_params = study.best_params

print(f"Best parameters: {best_params}")






import optuna
from catboost import CatBoostRegressor, Pool
from sklearn.metrics import make_scorer
import numpy as np
import pandas as pd

# Custom metric function for CatBoost
def top_1_percent_lift(y_true, y_pred_proba):
    top_1_percent = int(0.01 * len(y_true))
    top_indices = np.argsort(y_pred_proba)[-top_1_percent:]
    actual_positives = y_true.iloc[top_indices].sum()
    expected_positives = y_true.sum() * 0.01
    lift = actual_positives / expected_positives
    return lift

# Define the Optuna objective function with the custom evaluation metric
def objective(trial):
    train_pool = Pool(X_train, y_train)
    valid_pool = Pool(X_valid, y_valid)

    # Hyperparameters to be tuned
    param = {
        'iterations': trial.suggest_int('iterations', 100, 1000),
        # ... other hyperparameters ...
        'loss_function': 'Logloss',
        'eval_metric': 'AUC',
    }

    # Instantiate the CatBoost model
    catboost_model = CatBoostRegressor(**param)

    # Fit the model
    catboost_model.fit(train_pool, eval_set=valid_pool, verbose=0)

    # Make predictions (probability of the positive class for binary classification)
    y_pred_proba = catboost_model.predict(valid_pool, prediction_type='Probability')[:, 1]
    
    # Calculate the lift score
    lift_score = top_1_percent_lift(y_valid, y_pred_proba)

    return lift_score

# Assuming X_train, y_train, X_valid, y_valid are already defined and are your dataset splits
# train_pool = Pool(X_train, y_train)
# valid_pool = Pool(X_valid, y_valid)

# Create a study object
study = optuna.create_study(direction='maximize')

# Optimize the study, the objective function is called for each trial
study.optimize(objective, n_trials=50)

# Collecting the performance of all parameters tested into a dataframe
results = study.trials_dataframe(attrs=('number', 'value', 'params', 'state'))

# Saving the results to a CSV file
results.to_csv('catboost_optuna_results.csv', index=False)

# The best parameters found
best_params = study.best_params
best_lift_score = study.best_value



   if trial.suggest_categorical('handle_class_imbalance', [True, False]):
        param.update({
            'auto_class_weights': 'Balanced'
        })
    

print(f"Best parameters: {best_params}")
print(f"Best lift score: {best_lift_score}")


